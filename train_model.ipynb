{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\",quiet=True)\n",
    "sentences, y, labels = [], [], []\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        sentences.append(nltk.word_tokenize(pattern.lower()))\n",
    "        y.append(intent[\"tag\"])\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec = Word2Vec(sentences, min_count=1, vector_size=100)\n",
    "\n",
    "# Convert text to numerical vectors\n",
    "max_len = max([len(s) for s in sentences])\n",
    "X = np.zeros((len(sentences), max_len, 100))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        X[i, j] = word2vec.wv[word]\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_vec = label_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "c:\\Users\\eobri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "input_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n",
    "\n",
    "bert_output = bert_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
    "pooled_output = bert_output.pooler_output\n",
    "dense = Dense(len(labels), activation=\"softmax\")(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=dense)\n",
    "\n",
    "# Compile and train the model\n",
    "optimizer = Adam(lr=2e-5)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, tokenizer, max_len):\n",
    "    input_ids, input_mask, input_type_ids = [], [], []\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer(sent, padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "        input_ids.append(tokens[\"input_ids\"])\n",
    "        input_mask.append(tokens[\"attention_mask\"])\n",
    "        input_type_ids.append(tokens[\"token_type_ids\"])\n",
    "\n",
    "    return np.array(input_ids), np.array(input_mask), np.array(input_type_ids)\n",
    "\n",
    "X_tokenized = [\" \".join(s) for s in sentences]\n",
    "input_ids, input_mask, input_type_ids = tokenize(X_tokenized, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ids, X_test_ids, X_train_mask, X_test_mask, X_train_type_ids, X_test_type_ids, y_train, y_test = train_test_split(input_ids, input_mask, input_type_ids, y_vec, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 29s 2s/step - loss: 3.0032 - accuracy: 0.0588 - val_loss: 2.9014 - val_accuracy: 0.0588\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 14s 2s/step - loss: 2.7728 - accuracy: 0.1618 - val_loss: 2.7886 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 2.3919 - accuracy: 0.2500 - val_loss: 2.8161 - val_accuracy: 0.1176\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 14s 1s/step - loss: 2.1946 - accuracy: 0.2206 - val_loss: 2.5263 - val_accuracy: 0.2353\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 1.9329 - accuracy: 0.4118 - val_loss: 2.5153 - val_accuracy: 0.1176\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 1.8092 - accuracy: 0.4559 - val_loss: 2.3519 - val_accuracy: 0.1176\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 1.5998 - accuracy: 0.5294 - val_loss: 2.2160 - val_accuracy: 0.2941\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 13s 2s/step - loss: 1.4346 - accuracy: 0.6765 - val_loss: 2.0085 - val_accuracy: 0.4118\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 15s 2s/step - loss: 1.2610 - accuracy: 0.7353 - val_loss: 1.9259 - val_accuracy: 0.4118\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 1.0994 - accuracy: 0.7941 - val_loss: 1.9200 - val_accuracy: 0.4118\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.8879 - accuracy: 0.8529 - val_loss: 1.7360 - val_accuracy: 0.3529\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.8312 - accuracy: 0.8529 - val_loss: 1.6429 - val_accuracy: 0.2941\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.6616 - accuracy: 0.9412 - val_loss: 1.7433 - val_accuracy: 0.3529\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.5970 - accuracy: 0.9265 - val_loss: 1.7278 - val_accuracy: 0.4118\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.5057 - accuracy: 0.9559 - val_loss: 1.8887 - val_accuracy: 0.3529\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.4215 - accuracy: 0.9706 - val_loss: 1.5083 - val_accuracy: 0.4118\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.4022 - accuracy: 0.9853 - val_loss: 1.5485 - val_accuracy: 0.4706\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.2869 - accuracy: 1.0000 - val_loss: 1.6349 - val_accuracy: 0.4118\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.2576 - accuracy: 1.0000 - val_loss: 1.4412 - val_accuracy: 0.5882\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 13s 1s/step - loss: 0.2104 - accuracy: 1.0000 - val_loss: 1.4934 - val_accuracy: 0.5882\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.1748 - accuracy: 1.0000 - val_loss: 1.4813 - val_accuracy: 0.5882\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 13s 2s/step - loss: 0.1494 - accuracy: 1.0000 - val_loss: 1.3840 - val_accuracy: 0.5882\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.1222 - accuracy: 1.0000 - val_loss: 1.4379 - val_accuracy: 0.5882\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.1212 - accuracy: 1.0000 - val_loss: 1.5060 - val_accuracy: 0.5882\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 12s 1s/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 1.4790 - val_accuracy: 0.5882\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 15s 2s/step - loss: 0.0893 - accuracy: 1.0000 - val_loss: 1.4858 - val_accuracy: 0.5882\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 14s 2s/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 1.4910 - val_accuracy: 0.5882\n",
      "Epoch 27: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 50\n",
    "batch_size = 8\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_ids, X_train_mask, X_train_type_ids],\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([X_test_ids, X_test_mask, X_test_type_ids], y_test),callbacks=[early_stopping] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('telehealth_chatbot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eobri\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.utils import custom_object_scope\n",
    "from transformers import TFBertModel\n",
    "\n",
    "# register custom object for TFBertModel layer\n",
    "with custom_object_scope({'TFBertModel': TFBertModel}):\n",
    "    # load the saved model\n",
    "    model = load_model('telehealth_chatbot.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text, model, tokenizer, word2vec, label_encoder):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokenized_text = \" \".join(tokens)\n",
    "    input_ids, input_mask, input_type_ids = tokenize([tokenized_text], tokenizer, max_len)\n",
    "\n",
    "    probabilities = model.predict([input_ids, input_mask, input_type_ids])[0]\n",
    "    intent_idx = np.argmax(probabilities)\n",
    "\n",
    "    return label_encoder.inverse_transform([intent_idx])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    intent_data_chatbot = json.load(file)['intents']\n",
    "\n",
    "def get_response(user_input):\n",
    "    # predict user intent\n",
    "    intent = predict_intent(user_input, model, tokenizer, word2vec, label_encoder)\n",
    "\n",
    "    # map intent to response\n",
    "    for intent_data in intent_data_chatbot:\n",
    "        if intent_data['tag'] == intent:\n",
    "            response = random.choice(intent_data['responses'])\n",
    "            break\n",
    "    \n",
    "    return response\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mif\u001b[39;00m user_input\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mquit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m response \u001b[39m=\u001b[39m get_response(user_input)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBot: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_response\u001b[39m(user_input):\n\u001b[0;32m      5\u001b[0m     \u001b[39m# predict user intent\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     intent \u001b[39m=\u001b[39m predict_intent(user_input, model, tokenizer, word2vec, label_encoder)\n\u001b[0;32m      8\u001b[0m     \u001b[39m# map intent to response\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m intent_data \u001b[39min\u001b[39;00m intent_data_chatbot:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2vec' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    response = get_response(user_input)\n",
    "    print(f\"Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
